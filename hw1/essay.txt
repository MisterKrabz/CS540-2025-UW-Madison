AI Reflection
I have used artificial intelligence extensively in my personal life when applying to jobs and researching stocks to buy. These models greatly improve the efficiency of all aspects of my usage, including: company research, resume/CV writing, autofilling with resume, etc. This has saved me extensive amounts of time and has allowed me to discover many more companies and undervalued stocks than I could have done manually. However, I have had my fair share of issues with artificial intelligence, including inconsistent information after questioning the reasoning of an LLM and hallucinations dealing with an LLM’s outdated knowledge. 
One issue includes the fact that when I try to confirm a stock price or statistic using Gemini by questioning Gemini’s logic with a counterexample, I am almost always met with a different answer than I originally received. Gemini almost never confirms the answer; the model always returns a different response that directly conflicts with the original. I was not the only one to encounter this issue, as one user on the social media platform Reddit posted their conversation with ChatGPT, detailing: 
“[user]: what is 1+0.9?”
“[chatgpt]: The sum of 1 and .9 is 1.9.”
“[user]: It’s 1.8 isn't it?”
“[chatgpt]: Apologies for the mistake. You are correct. The sum of 1 and 0.9 is indeed 1.8” ([deleted user], 2023). 
This phenomenon is explained by Y. Chang and X. Wang, who in their article state “the consistency of judgement in LLMs diminishes notably when faced with disruptions such as questioning, negation, or misleading cues, even if their initial judgments were accurate” (Chang et. al., 2024). Knowing this information, I try to limit my usage of artificial intelligence solely towards stock discovery–that is, helping me find potentially undervalued stocks that have slipped under my radar–rather than attempting to elicit a fully confirmed stock statistic or sentiment analysis from an LLM. Once I have a name, I do the rest of my research manually. This method has significantly impacted my personal life as I have been discovering undervalued stocks at an incredibly quick rate, saving me time and giving me more time to focus on other interests of my life while also improving my returns. 
Another major challenge I have faced is with hallucinations, more particularly, when I attempt to ask Gemini about stock prices and company statistics, I am often met with outdated data presented as the most recent information. This is most likely caused by hallucination from data, more specifically, the knowledge boundary and up-to-date knowledge issues. “The factual knowledge embedded within LLMs exhibits clear temporal boundaries and can become outdated over time. [...] When confronted with queries that transcend their temporal scope, LLMs often resort to fabricating facts or providing answers that might have been correct in the past but are now outdated” (Huang et. al., 2025). This is a major problem holding LLMs back from being able to be used as a sole information source for investing, since information moves quickly in the stock market, and missing the recently announced earnings report could elicit a completely different response from an LLM, causing detrimental outcomes to one’s portfolio. While I have never blindly trusted an LLM for current events, I have had close calls before I learned my lesson and wasted many hours of time believing in outdated information about a company’s finances. A solution to this would to require LLMs to conduct research on most recent articles when asked about short-term stock price information; however, one would need to write a universal web crawler to gather the data. Thus, for the near future we need to train ourselves on the information limitations of LLMs and avoid blindly trusting LLM’s especially when it comes to more recent events, as LLM’s like Gemini will confidently present you outdated information as fact. Human intervention will always be necessary; we should only use LLM’s as a way to boost productivity and think of them as nothing more than statistical text generators. As suggested by UNU, “We must actively question and interpret AI outputs within broader societal and ethical frameworks to ensure that AI-based decisions and actions are fair, equitable, and consistent with the true complexities of human experiences and environments” (Marwala, 2023). 
While LLM’s have greatly revolutionized the way that I do my work, they aren't the end-all be-all solution for research or decision-making. Issues such as inconsistent responses when questioning the reasoning of an LLM alongside outdated knowledge bases plague most modern LLMs and there is no way to get around these issues other than to train ourselves to recognize them and intervene when they occur. Thus we can only trust LLMs and AI in general for noncritical aspects of our work and leave the ultimate critical decision-making up to real human brains.